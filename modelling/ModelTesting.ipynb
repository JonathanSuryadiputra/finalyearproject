{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e140d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaab6329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\unumuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\unumuser\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc9e8ec",
   "metadata": {},
   "source": [
    "Unseen sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "865cb154",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = ['I find this course somewhat okay, but it still needs improvements. Other than that it is fine.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffec0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define preprocessing function to remove stopwords and lemmatise the text\n",
    "def remove_stopwords_lemmatise(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    words = [token.lemma_ for token in nlp(text) if not token.is_punct]\n",
    "    words = [word.lower() for word in words if word.lower() not in stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08ac59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_processed = []\n",
    "for x in raw_text:\n",
    "    text = x.lower()\n",
    "    text = x.strip()\n",
    "    text = re.sub(r'\\d+', '', x)\n",
    "    text = re.sub(r'<br>', '', x)\n",
    "    text = re.sub(r'<br />', '',x)\n",
    "    text = re.sub(r'[^\\w\\s]', '',x)\n",
    "    text = re.sub(r' +', ' ', x)\n",
    "    text = remove_stopwords_lemmatise(x)\n",
    "    raw_text_processed.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed667d1",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da6431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pickle.load(open('multinomial_nb.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2d5a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = pickle.load(open('vectoriser.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017a3948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unseen_dtm = vect.transform(raw_text_processed)\n",
    "text_predict = nb.predict(x_unseen_dtm)\n",
    "text_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af988e6f",
   "metadata": {},
   "source": [
    "# LSTM RNN Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a30931",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('lstm2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f99424",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 50000 # make the top list of words (common words)\n",
    "embedding_dim = 64\n",
    "max_len = 600\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<OOV>' # OOV = Out of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7693207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pickle.load(open('tokeniser.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a84b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.8705384e-10 1.9373713e-03 7.2418556e-02 8.6108887e-01 5.4922651e-02\n",
      "  9.6324999e-03]] 3\n"
     ]
    }
   ],
   "source": [
    "seq = tokenizer.texts_to_sequences(raw_text_processed)\n",
    "padded = pad_sequences(seq, maxlen=max_len, padding=padding_type, truncating=trunc_type)\n",
    "pred = model.predict(padded)\n",
    "print(pred, np.argmax(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
